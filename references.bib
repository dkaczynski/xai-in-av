@article{Interlandi2017,
author = {Interlandi, Matteo and Ekmekji, Ari and Shah, Kshitij and Gulzar, Muhammad Ali and Tetali, Sai Deep and Kim, Miryung and Millstein, Todd and Condie, Tyson},
journal = {VLDB Journal},
number = {5},
pages = {1--21},
title = {{Adding data provenance support to Apache Spark}},
volume = {27},
year = {2017}
}

@inproceedings{Ma2017,
 author = {Ma, Shiqing and Aafer, Yousra and Xu, Zhaogui and Lee, Wen-Chuan and Zhai, Juan and Liu, Yingqi and Zhang, Xiangyu},
 title = {{LAMP}: Data Provenance for Graph Based Machine Learning Algorithms Through Derivative Computation},
 booktitle = {Proceedings of the 2017 11th Joint Meeting on Foundations of Software Engineering},
 year = {2017},
 pages = {786--797}
} 

@article{Curcin2017,
author = {Curcin, Vasa and Fairweather, Elliot and Danger, Roxana and Corrigan, Derek},
journal = {Journal of Biomedical Informatics},
pages = {1--21},
title = {{Templates as a method for implementing data provenance in decision support systems}},
volume = {65},
year = {2017}
}

@article{MONTAVON20181,
title = {{Methods for interpreting and understanding deep neural networks}},
journal = "Digital Signal Processing",
volume = "73",
pages = "1 - 15",
year = "2018",
issn = "1051-2004",
doi = "https://doi.org/10.1016/j.dsp.2017.10.011",
url = "http://www.sciencedirect.com/science/article/pii/S1051200417302385",
author = "Grégoire Montavon and Wojciech Samek and Klaus-Robert Müller",
keywords = "Deep neural networks, Activation maximization, Sensitivity analysis, Taylor decomposition, Layer-wise relevance propagation",
abstract = "This paper provides an entry point to the problem of interpreting a deep neural network model and explaining its predictions. It is based on a tutorial given at ICASSP 2017. As a tutorial paper, the set of methods covered here is not exhaustive, but sufficiently representative to discuss a number of questions in interpretability, technical challenges, and possible applications. The second part of the tutorial focuses on the recently proposed layer-wise relevance propagation (LRP) technique, for which we provide theory, recommendations, and tricks, to make most efficient use of it on real data."
}

@article{Lapuschkin2016,
abstract = {The Layer-wise Relevance Propagation (LRP) algorithm explains a classifier's prediction specific to a given data point by attributing relevance scores to important components of the input by using the topology of the learned model itself. With the LRP Toolbox we provide platform-agnostic implementations for explaining the predictions of pre-trained state of the art Caffe networks and stand-alone implementations for fully connected Neural Network models. The implementations for Matlab and python shall serve as a playing field to familiarize oneself with the LRP algorithm and are implemented with readability and transparency in mind. Models and data can be imported and exported using raw text formats, Matlab's .mat files and the .npy format for numpy or plain text.},
author = {Lapuschkin, Sebastian and Binder, Alexander and M{\"{u}}ller, Klaus- Robert and Samek, Wojciech},
journal = {Journal of Machine Learning Research},
title = {{The LRP Toolbox for Artificial Neural Networks}},
url = {http://www.mendeley.com/research/lrp-toolbox-artificial-neural-networks},
year = {2016}
}

@inproceedings{Liu:2018:ADM:3219819.3220027,
 author = {Liu, Ninghao and Yang, Hongxia and Hu, Xia},
 title = {{Adversarial Detection with Model Interpretation}},
 booktitle = {Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery \& Data Mining},
 series = {KDD '18},
 year = {2018},
 isbn = {978-1-4503-5552-0},
 location = {London, United Kingdom},
 pages = {1803--1811},
 numpages = {9},
 url = {http://doi.acm.org/10.1145/3219819.3220027},
 doi = {10.1145/3219819.3220027},
 acmid = {3220027},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {adversarial detection, machine learning interpretation, spammer detection},
} 

@article{Wexler.2017,
 author  = {Wexler, Rebecca},
 date    = {2017-06-13},
 title   = {{When a Computer Program Keeps You in Jail}},
 journal = {The New York Times},
 url     = {https://www.nytimes.com/2017/06/13/opinion/how-computers-are-harming-criminal-justice.html},
 urldate = {2019-02-03},
 month = jun,
 year = {2017},
}

@article{Angwin2016,
author = {Angwin, Jula and Larson Jeff and Matta, Surya and Kirchner, Lauren},
date = {2016-05-23},
title = {Machine Bias},
journal = {Pro Publica},
year = {2016},
url = {https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing},
urldate = {2019-02-09},
}

@article{Hohman2018,
abstract = {Deep learning has recently seen rapid development and received significant attention due to its state-of-the-art performance on previously-thought hard problems. However, because of the internal complexity and nonlinear structure of deep neural networks, the underlying decision making processes for why these models are achieving such performance are challenging and sometimes mystifying to interpret. As deep learning spreads across domains, it is of paramount importance that we equip users of deep learning with tools for understanding when a model works correctly, when it fails, and ultimately how to improve its performance. Standardized toolkits for building neural networks have helped democratize deep learning; visual analytics systems have now been developed to support model explanation, interpretation, debugging, and improvement. We present a survey of the role of visual analytics in deep learning research, which highlights its short yet impactful history and thoroughly summarizes the state-of-the-art using a human-centered interrogative framework, focusing on the Five W's and How (Why, Who, What, How, When, and Where). We conclude by highlighting research directions and open research problems. This survey helps researchers and practitioners in both visual analytics and deep learning to quickly learn key aspects of this young and rapidly growing body of research, whose impact spans a diverse range of domains.},
archivePrefix = {arXiv},
arxivId = {1801.06889},
author = {Hohman, Fred Matthew and Kahng, Minsuk and Pienta, Robert and Chau, Duen Horng},
doi = {10.1109/TVCG.2018.2843369},
eprint = {1801.06889},
isbn = {10772626 (ISSN)},
issn = {10772626},
journal = {IEEE Transactions on Visualization and Computer Graphics},
keywords = {DNN,Deep learning,information visualization,neural networks,survey,visual analytics,visualization},
mendeley-groups = {Surveys},
mendeley-tags = {DNN,neural networks,survey,visualization},
number = {1},
pages = {1--20},
title = {{Visual Analytics in Deep Learning: An Interrogative Survey for the Next Frontiers}},
volume = {25},
year = {2018}
}

@inproceedings{Ribeiro:2016:WIT:2939672.2939778,
 author = {Ribeiro, Marco Tulio and Singh, Sameer and Guestrin, Carlos},
 title = {{\"Why Should I Trust You?\": Explaining the Predictions of Any Classifier}},
 booktitle = {Proceedings of the 22Nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
 series = {KDD '16},
 year = {2016},
 isbn = {978-1-4503-4232-2},
 location = {San Francisco, California, USA},
 pages = {1135--1144},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/2939672.2939778},
 doi = {10.1145/2939672.2939778},
 acmid = {2939778},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {black box classifier, explaining machine learning, interpretability, interpretable machine learning},
} 

@article{Guidotti:2018:SME:3271482.3236009,
 author = {Guidotti, Riccardo and Monreale, Anna and Ruggieri, Salvatore and Turini, Franco and Giannotti, Fosca and Pedreschi, Dino},
 title = {{A Survey of Methods for Explaining Black Box Models}},
 journal = {ACM Comput. Surv.},
 issue_date = {January 2019},
 volume = {51},
 number = {5},
 month = aug,
 year = {2018},
 issn = {0360-0300},
 pages = {93:1--93:42},
 articleno = {93},
 numpages = {42},
 url = {http://doi.acm.org/10.1145/3236009},
 doi = {10.1145/3236009},
 acmid = {3236009},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {Open the black box, explanations, interpretability, transparent models},
} 

@inproceedings{Veale:2018:FAD:3173574.3174014,
 author = {Veale, Michael and Van Kleek, Max and Binns, Reuben},
 title = {{Fairness and Accountability Design Needs for Algorithmic Support in High-Stakes Public Sector Decision-Making}},
 booktitle = {Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems},
 series = {CHI '18},
 year = {2018},
 isbn = {978-1-4503-5620-6},
 location = {Montreal QC, Canada},
 pages = {440:1--440:14},
 articleno = {440},
 numpages = {14},
 url = {http://doi.acm.org/10.1145/3173574.3174014},
 doi = {10.1145/3173574.3174014},
 acmid = {3174014},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {algorithmic accountability, algorithmic bias, decision-support, predictive policing, public administration},
}

@article{10.1371/journal.pone.0130140,
    author = {Bach, Sebastian AND Binder, Alexander AND Montavon, Grégoire AND Klauschen, Frederick AND Müller, Klaus-Robert AND Samek, Wojciech},
    journal = {PLOS ONE},
    publisher = {Public Library of Science},
    title = {On Pixel-Wise Explanations for Non-Linear Classifier Decisions by Layer-Wise Relevance Propagation},
    year = {2015},
    month = {07},
    volume = {10},
    url = {https://doi.org/10.1371/journal.pone.0130140},
    pages = {1-46},
    abstract = {Understanding and interpreting classification decisions of automated image classification systems is of high value in many applications, as it allows to verify the reasoning of the system and provides additional information to the human expert. Although machine learning methods are solving very successfully a plethora of tasks, they have in most cases the disadvantage of acting as a black box, not providing any information about what made them arrive at a particular decision. This work proposes a general solution to the problem of understanding classification decisions by pixel-wise decomposition of nonlinear classifiers. We introduce a methodology that allows to visualize the contributions of single pixels to predictions for kernel-based classifiers over Bag of Words features and for multilayered neural networks. These pixel contributions can be visualized as heatmaps and are provided to a human expert who can intuitively not only verify the validity of the classification decision, but also focus further analysis on regions of potential interest. We evaluate our method for classifiers trained on PASCAL VOC 2009 images, synthetic image data containing geometric shapes, the MNIST handwritten digits data set and for the pre-trained ImageNet model available as part of the Caffe open source package.},
    number = {7},
    doi = {10.1371/journal.pone.0130140}
}

@article{Goodman2017EuropeanUR,
  title={European Union Regulations on Algorithmic Decision-Making and a “Right to Explanation”},
  author={Bryce Goodman and Seth Flaxman},
  journal={AI Magazine},
  year={2017},
  volume={38},
  pages={50-57}
}

@inproceedings{Bibal2016,
author = {Bibal, Adrien and Frénay, Benoît},
year = {2016},
month = {04},
pages = {},
title = {Interpretability of Machine Learning Models and Representations: an Introduction},
booktitle = {European Symposium on Artificial Neural Networks, Computational Intelligence and Machine Learning},
 }

@InProceedings{10.1007/978-3-319-46493-0_1,
author="Hendricks, Lisa Anne
and Akata, Zeynep
and Rohrbach, Marcus
and Donahue, Jeff
and Schiele, Bernt
and Darrell, Trevor",
editor="Leibe, Bastian
and Matas, Jiri
and Sebe, Nicu
and Welling, Max",
title="Generating Visual Explanations",
booktitle="Computer Vision -- ECCV 2016",
year="2016",
publisher="Springer International Publishing",
address="Cham",
pages="3--19",
abstract="Clearly explaining a rationale for a classification decision to an end user can be as important as the decision itself. Existing approaches for deep visual recognition are generally opaque and do not output any justification text; contemporary vision-language models can describe image content but fail to take into account class-discriminative image aspects which justify visual predictions. We propose a new model that focuses on the discriminating properties of the visible object, jointly predicts a class label, and explains why the predicted label is appropriate for the image. Through a novel loss function based on sampling and reinforcement learning, our model learns to generate sentences that realize a global sentence property, such as class specificity. Our results on the CUB dataset show that our model is able to generate explanations which are not only consistent with an image but also more discriminative than descriptions produced by existing captioning methods.",
isbn="978-3-319-46493-0"
}

@inproceedings{Nguyen2016,
archivePrefix = {arXiv},
arxivId = {1605.09304},
author = {Nguyen, Anh and Dosovitskiy, Alexey and Yosinski, Jason and Brox, Thomas and Clune, Jeff},
booktitle = {NIPS},
doi = {10.1098/rstb.2009.0091},
eprint = {1605.09304},
file = {:Users/david/Library/Application Support/Mendeley Desktop/Downloaded/Nguyen et al. - 2016 - Synthesizing the preferred inputs for neurons in neural networks via deep generator networks.pdf:pdf},
issn = {10495258},
keywords = {DNN,activation maximization,interpretability},
mendeley-tags = {DNN,activation maximization,interpretability},
title = {{Synthesizing the preferred inputs for neurons in neural networks via deep generator networks}},
year = {2016}
}

@inproceedings{Abdul:2018:TTE:3173574.3174156,
 author = {Abdul, Ashraf and Vermeulen, Jo and Wang, Danding and Lim, Brian Y. and Kankanhalli, Mohan},
 title = {Trends and Trajectories for Explainable, Accountable and Intelligible Systems: An {HCI} Research Agenda},
 booktitle = {Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems},
 series = {CHI '18},
 year = {2018},
 isbn = {978-1-4503-5620-6},
 location = {Montreal QC, Canada},
 pages = {582:1--582:18},
 articleno = {582},
 numpages = {18},
 url = {http://doi.acm.org/10.1145/3173574.3174156},
 doi = {10.1145/3173574.3174156},
 acmid = {3174156},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {explainable artificial intelli-gence, explanations, intelligibility, interpretable machine learning},
} 

@article{Berendt2014,
author={Berendt, Bettina and Preibusch, S{\"o}ren},
title={{Better decision support through exploratory discrimination-aware data mining: foundations and empirical evidence}},
journal="Artificial Intelligence and Law",
year="2014",
month="Jun",
day="01",
volume="22",
number="2",
pages="175--209",
abstract="Decision makers in banking, insurance or employment mitigate many of their risks by telling ``good'' individuals and ``bad'' individuals apart. Laws codify societal understandings of which factors are legitimate grounds for differential treatment (and when and in which contexts)---or are considered unfair discrimination, including gender, ethnicity or age. Discrimination-aware data mining (DADM) implements the hope that information technology supporting the decision process can also keep it free from unjust grounds. However, constraining data mining to exclude a fixed enumeration of potentially discriminatory features is insufficient. We argue for complementing it with exploratory DADM, where discriminatory patterns are discovered and flagged rather than suppressed. This article discusses the relative merits of constraint-oriented and exploratory DADM from a conceptual viewpoint. In addition, we consider the case of loan applications to empirically assess the fitness of both discrimination-aware data mining approaches for two of their typical usage scenarios: prevention and detection. Using Mechanical Turk, 215 US-based participants were randomly placed in the roles of a bank clerk (discrimination prevention) or a citizen / policy advisor (detection). They were tasked to recommend or predict the approval or denial of a loan, across three experimental conditions: discrimination-unaware data mining, exploratory, and constraint-oriented DADM (eDADM resp. cDADM). The discrimination-aware tool support in the eDADM and cDADM treatments led to significantly higher proportions of correct decisions, which were also motivated more accurately. There is significant evidence that the relative advantage of discrimination-aware techniques depends on their intended usage. For users focussed on making and motivating their decisions in non-discriminatory ways, cDADM resulted in more accurate and less discriminatory results than eDADM. For users focussed on monitoring for preventing discriminatory decisions and motivating these conclusions, eDADM yielded more accurate results than cDADM.",
issn="1572-8382",
doi="10.1007/s10506-013-9152-0",
url="https://doi.org/10.1007/s10506-013-9152-0"
}

@article{DBLP:journals/corr/abs-1711-00399,
  author    = {Sandra Wachter and
               Brent D. Mittelstadt and
               Chris Russell},
  title     = {Counterfactual Explanations without Opening the Black Box: Automated
               Decisions and the {GDPR}},
  journal   = {CoRR},
  volume    = {abs/1711.00399},
  year      = {2017},
  url       = {http://arxiv.org/abs/1711.00399},
  archivePrefix = {arXiv},
  eprint    = {1711.00399},
  timestamp = {Wed, 23 Jan 2019 13:31:00 +0100},
  biburl    = {https://dblp.org/rec/bib/journals/corr/abs-1711-00399},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{Adadi2018, 
author={A. Adadi and M. Berrada}, 
journal={IEEE Access}, 
title={Peeking Inside the Black-Box: A Survey on Explainable Artificial Intelligence (XAI)}, 
year={2018}, 
volume={6}, 
number={}, 
pages={52138-52160}, 
keywords={artificial intelligence;AI-based systems;black-box nature;explainable AI;XAI;explainable artificial intelligence;fourth industrial revolution;Conferences;Machine learning;Market research;Prediction algorithms;Machine learning algorithms;Biological system modeling;Explainable artificial intelligence;interpretable machine learning;black-box models}, 
doi={10.1109/ACCESS.2018.2870052}, 
ISSN={2169-3536}, 
month={},
}

@article{Read2016,
author = {Read, Richard},
date = {2016-03-01},
year = {2016},
title = {For the first time, Google admits its autonomous car is at fault in fender-bender},
journal = {Washington Post},
url = {https://www.washingtonpost.com/cars/for-the-first-time-google-admits-its-autonomous-car-is-at-fault-in-fender-bender/2016/03/01/356445c8-e009-11e5-8c00-8aa03741dced_story.html},
urldate = {2019-02-10},
}

@misc{H20.ai,
  title = {{Overview -- H20.ai 3.24.0.1 documentation}},
  howpublished = {http://docs.h2o.ai/h2o/latest-stable/h2o-docs/index.html},
  urldate = {2019-04-03}
}

@misc{Tesla2018,
author = {{The Tesla Team}},
title = {An Update on Last Week’s Accident},
journal = {Tesla Blog},
type = {Blog},
date = {2018-03-03},
number = {March 3},
year = {2018},
howpublished = {https://www.tesla.com/blog/update-last-week%E2%80%99s-accident},
urldate = {2019-02-10}
}

@article{Ackerman2016,
author = {Ackerman, Evan},
title = {Fatal Tesla Self-Driving Car Crash Reminds Us That Robots Aren't Perfect},
journal = {{IEEE} Spectrum},
date = {2016-06-01},
year = {2016},
url = {https://spectrum.ieee.org/cars-that-think/transportation/self-driving/fatal-tesla-autopilot-crash-reminds-us-that-robots-arent-perfect},
urldate = {2019-02-10}
}

@techreport{Bhavsar2017,
author = {Bhavsar, Parth and Dey, Kakan and Chowdhury, Mashrur and Das, Plaban},
title = {{Risk Analysis of Autonomous Vehicles in Mixed Traffic Streams}},
year = {2017}
}

@inproceedings{Schelter2017,
  author    = {Schelter, Sebastian and B{\"o}se, Joos-Hendrik and Klein, Thoralf and Seufert, Stephan},
  title     = {{Automatically Tracking Metadata and Provenance of Machine Learning Experiments}},
  year      = {2017},
  maintitle = {Conference on Neural Information Processing Systems},
  booktitle = {Machine Learning Systems Workshop},
}

@Article{Koo2015,
author={Koo, Jeamin
and Kwac, Jungsuk
and Ju, Wendy
and Steinert, Martin
and Leifer, Larry
and Nass, Clifford},
title={Why did my car just do that? Explaining semi-autonomous driving actions to improve driver understanding, trust, and performance},
journal={International Journal on Interactive Design and Manufacturing (IJIDeM)},
year={2015},
month={Nov},
day={01},
volume={9},
number={4},
pages={269--275},
abstract={This study explores, in the context of semi-autonomous driving, how the content of the verbalized message accompanying the car's autonomous action affects the driver's attitude and safety performance. Using a driving simulator with an auto-braking function, we tested different messages that provided advance explanation of the car's imminent autonomous action. Messages providing only ``how'' information describing actions (e.g., ``The car is braking'') led to poor driving performance, whereas ``why'' information describing reasoning for actions (e.g., ``Obstacle ahead'') was preferred by drivers and led to better driving performance. Providing both ``how and why'' resulted in the safest driving performance but increased negative feelings in drivers. These results suggest that, to increase overall safety, car makers need to attend not only to the design of autonomous actions but also to the right way to explain these actions to the drivers.},
issn={1955-2505},
doi={10.1007/s12008-014-0227-2},
url={https://doi.org/10.1007/s12008-014-0227-2}
}

@article{DBLP:journals/corr/abs-1812-00891,
  author    = {Xinyang Zhang and
               Ningfei Wang and
               Shouling Ji and
               Hua Shen and
               Ting Wang},
  title     = {Interpretable Deep Learning under Fire},
  journal   = {CoRR},
  volume    = {abs/1812.00891},
  year      = {2018},
  url       = {http://arxiv.org/abs/1812.00891},
  archivePrefix = {arXiv},
  eprint    = {1812.00891},
  timestamp = {Tue, 01 Jan 2019 15:01:25 +0100},
  biburl    = {https://dblp.org/rec/bib/journals/corr/abs-1812-00891},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{Chen2018,
  author    = {Chaofan Chen and
               Oscar Li and
               Alina Barnett and
               Jonathan Su and
               Cynthia Rudin},
  title     = {This looks like that: deep learning for interpretable image recognition},
  journal   = {CoRR},
  volume    = {abs/1806.10574},
  year      = {2018},
  url       = {http://arxiv.org/abs/1806.10574},
  archivePrefix = {arXiv},
  eprint    = {1806.10574},
  timestamp = {Mon, 13 Aug 2018 16:47:37 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/abs-1806-10574},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{Werbos1974,
author = {Werbos, Paul and J. (Paul John, Paul},
year = {1974},
month = {01},
pages = {},
title = {Beyond regression : new tools for prediction and analysis in the behavioral sciences /}
}

@article{kim2018textual,
  title={Textual Explanations for Self-Driving Vehicles},
  author={Kim, Jinkyu and Rohrbach, Anna and Darrell, Trevor and Canny, John and Akata, Zeynep},
  journal={Proceedings of the European Conference on Computer Vision (ECCV)},
  year={2018}
}

@article{Kim2017InterpretableLF,
  title={Interpretable Learning for Self-Driving Cars by Visualizing Causal Attention},
  author={Jinkyu Kim and John F. Canny},
  journal={2017 IEEE International Conference on Computer Vision (ICCV)},
  year={2017},
  pages={2961-2969}
}

@inproceedings{Kim2017ShowA,
  title={Show , Attend , Control , and Justify : Interpretable Learning for Self-Driving Cars},
  author={Jinkyu Kim and Anna Rohrbach and Trevor Darrell and John F. Canny and Zeynep Akata},
  year={2017}
}

@article{Bojarski2017ExplainingHA,
  title={Explaining How a Deep Neural Network Trained with End-to-End Learning Steers a Car},
  author={Mariusz Bojarski and Philip Yeres and Anna Choromanska and Krzysztof Choromanski and Bernhard Firner and Lawrence D. Jackel and Urs Muller},
  journal={CoRR},
  year={2017},
  volume={abs/1704.07911}
}

@article{BachBMMS15,
  author    = {Sebastian Bach and
               Alexander Binder and
               Gr{\'{e}}goire Montavon and
               Klaus{-}Robert M{\"{u}}ller and
               Wojciech Samek},
  title     = {Analyzing Classifiers: Fisher Vectors and Deep Neural Networks},
  journal   = {CoRR},
  volume    = {abs/1512.00172},
  year      = {2015},
  url       = {http://arxiv.org/abs/1512.00172},
  archivePrefix = {arXiv},
  eprint    = {1512.00172},
  timestamp = {Mon, 13 Aug 2018 16:47:18 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/BachBMMS15},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{Bach2016AnalyzingCF,
  title={Analyzing Classifiers: Fisher Vectors and Deep Neural Networks},
  author={Sebastian Bach and Alexander Binder and Gr{\'e}goire Montavon and Klaus-Robert M{\"u}ller and Wojciech Samek},
  journal={2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  year={2016},
  pages={2912-2920}
}

@article{Zaharia2018,
abstract = {Machine learning development creates multiple new challenges that are not present in a traditional software development lifecycle. These include keeping track of the myriad inputs to an ML application (e.g., data versions, code and tuning parameters), reproducing results, and production deployment. In this paper, we summarize these challenges from our experience with Databricks customers, and describe MLflow, an open source platform we recently launched to streamline the machine learning lifecycle. MLflow covers three key challenges: experimentation, reproducibility, and model deployment, using generic APIs that work with any ML library, algorithm and programming language. The project has a rapidly growing open source community, with over 50 contributors since its launch in June 2018.},
author = {Zaharia, Matei and Chen, Andrew and Davidson, Aaron and Ghodsi, Ali and Hong, Sue Ann and Konwinski, Andy and Murching, Siddharth and Nykodym, Tomas and Ogilvie, Paul and Parkhe, Mani and Xie, Fen and Zumar, Corey},
keywords = {data provenance},
pages = {39--45},
title = {{Accelerating the Machine Learning Lifecycle with MLflow}},
year = {2018},
journal = {IEEE Technical Committee on Data Engineering},
}

@article{Simmhan:2005:SDP:1084805.1084812,
 author = {Simmhan, Yogesh L. and Plale, Beth and Gannon, Dennis},
 title = {A Survey of Data Provenance in e-Science},
 journal = {SIGMOD Rec.},
 issue_date = {September 2005},
 volume = {34},
 number = {3},
 month = sep,
 year = {2005},
 issn = {0163-5808},
 pages = {31--36},
 numpages = {6},
 url = {http://doi.acm.org/10.1145/1084805.1084812},
 doi = {10.1145/1084805.1084812},
 acmid = {1084812},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Simmhan2005ASO,
  title={A survey of data provenance techniques},
  author={Yogesh L. Simmhan and Beth Plale and Dennis Gannon},
  year={2005}
}

@article{Mittelstadt2017,
    author = {Mittelstadt, Brent and Floridi, Luciano and Wachter, Sandra},
    title = "{Why a Right to Explanation of Automated Decision-Making Does Not Exist in the General Data Protection Regulation}",
    journal = {International Data Privacy Law},
    volume = {7},
    number = {2},
    pages = {76-99},
    year = {2017},
    month = {06},
    abstract = "{Key PointsSince approval of the European Union General Data Protection Regulation (GDPR) in 2016, it has been widely and repeatedly claimed that a ‘right to explanation’ of all decisions made by automated or artificially intelligent algorithmic systems will be legally mandated by the GDPR once it is in force, in 2018.However, there are several reasons to doubt both the legal existence and the feasibility of such a right. In contrast to the right to explanation of specific automated decisions claimed elsewhere, the GDPR only mandates that data subjects receive meaningful, but properly limited, information (Articles 13–15) about the logic involved, as well as the significance and the envisaged consequences of automated decision-making systems, what we term a ‘right to be informed’.The ambiguity and limited scope of the ‘right not to be subject to automated decision-making’ contained in Article 22 (from which the alleged ‘right to explanation’ stems) raises questions over the protection actually afforded to data subjects.These problems show that the GDPR lacks precise language as well as explicit and well-defined rights and safeguards against automated decision-making, and therefore runs the risk of being toothless.We propose a number of legislative steps that, if implemented, may improve the transparency and accountability of automated decision-making when the GDPR comes into force in 2018.}",
    issn = {2044-3994},
    doi = {10.1093/idpl/ipx005},
    url = {https://doi.org/10.1093/idpl/ipx005},
    eprint = {http://oup.prod.sis.lan/idpl/article-pdf/7/2/76/17932196/ipx005.pdf},
}

@article{DBLP:journals/corr/BojarskiCCFJMZ16,
  author    = {Mariusz Bojarski and
               Anna Choromanska and
               Krzysztof Choromanski and
               Bernhard Firner and
               Larry D. Jackel and
               Urs Muller and
               Karol Zieba},
  title     = {{VisualBackProp: visualizing CNNs for autonomous driving}},
  journal   = {CoRR},
  volume    = {abs/1611.05418},
  year      = {2016},
  url       = {http://arxiv.org/abs/1611.05418},
  archivePrefix = {arXiv},
  eprint    = {1611.05418},
  timestamp = {Mon, 13 Aug 2018 16:48:06 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/BojarskiCCFJMZ16},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{Choi2015InvestigatingTI,
  title={Investigating the Importance of Trust on Adopting an Autonomous Vehicle},
  author={Jong Kyu Choi and Yong Gu Ji},
  journal={International Journal of Human Computer Interaction},
  year={2015},
  volume={31},
  pages={692-702}
}

@article{Ras2018ExplanationMI,
  title={Explanation Methods in Deep Learning: Users, Values, Concerns and Challenges},
  author={Gabrielle Ras and Marcel van Gerven and Willem F. G. Haselager},
  journal={CoRR},
  year={2018},
  volume={abs/1803.07517}
}

@article{Shailaja2018MachineLI,
  title={Machine Learning in Healthcare: A Review},
  author={Kola Shailaja and B. Seetharamulu and M. Akhil Jabbar},
  journal={2018 Second International Conference on Electronics, Communication and Aerospace Technology (ICECA)},
  year={2018},
  pages={910-914}
}

@ARTICLE{Zhang2011, 
author={J. {Zhang} and F. {Wang} and K. {Wang} and W. {Lin} and X. {Xu} and C. {Chen}}, 
journal={IEEE Transactions on Intelligent Transportation Systems}, 
title={Data-Driven Intelligent Transportation Systems: A Survey}, 
year={2011}, 
volume={12}, 
number={4}, 
pages={1624-1639}, 
keywords={automated highways;learning (artificial intelligence);security of data;data-driven intelligent transportation systems;travel security;technology-driven system;learning algorithm;privacy-aware people-centric more intelligent system;D2ITS;Data mining;Machine learning;Visualization;Visual analytics;Data mining;data-driven intelligent transportation systems ( $\hbox{D}^{2}\hbox{ITS}$);machine learning;microblog;mobility;visual analytics;visualization}, 
doi={10.1109/TITS.2011.2158001}, 
ISSN={1524-9050}, 
month={Dec},}

@article{LU20171,
title = "Industry 4.0: A survey on technologies, applications and open research issues",
journal = "Journal of Industrial Information Integration",
volume = "6",
pages = "1 - 10",
year = "2017",
issn = "2452-414X",
doi = "https://doi.org/10.1016/j.jii.2017.04.005",
url = "http://www.sciencedirect.com/science/article/pii/S2452414X17300043",
author = "Yang Lu",
keywords = "Industry 4.0, Cyber physical system, Internet of things, Big data, Enterprise architecture, Enterprise integration",
abstract = "Originally initiated in Germany, Industry 4.0, the fourth industrial revolution, has attracted much attention in recent literatures. It is closely related with the Internet of Things (IoT), Cyber Physical System (CPS), information and communications technology (ICT), Enterprise Architecture (EA), and Enterprise Integration (EI). Despite of the dynamic nature of the research on Industry 4.0, however, a systematic and extensive review of recent research on it is has been unavailable. Accordingly, this paper conducts a comprehensive review on Industry 4.0 and presents an overview of the content, scope, and findings of Industry 4.0 by examining the existing literatures in all of the databases within the Web of Science. Altogether, 88 papers related to Industry 4.0 are grouped into five research categories and reviewed. In addition, this paper outlines the critical issue of the interoperability of Industry 4.0, and proposes a conceptual framework of interoperability regarding Industry 4.0. Challenges and trends for future research on Industry 4.0 are discussed."
}

@article{GHANI2018,
title = "Social media big data analytics: A survey",
journal = "Computers in Human Behavior",
year = "2018",
issn = "0747-5632",
doi = "https://doi.org/10.1016/j.chb.2018.08.039",
url = "http://www.sciencedirect.com/science/article/pii/S074756321830414X",
author = "Norjihan Abdul Ghani and Suraya Hamid and Ibrahim Abaker Targio Hashem and Ejaz Ahmed",
keywords = "Big data, Social media, Machine learning, Analytics",
abstract = "Big data analytics has recently emerged as an important research area due to the popularity of the Internet and the advent of the Web 2.0 technologies. Moreover, the proliferation and adoption of social media applications have provided extensive opportunities and challenges for researchers and practitioners. The massive amount of data generated by users using social media platforms is the result of the integration of their background details and daily activities. This enormous volume of generated data known as “big data” has been intensively researched recently. A review of the recent works is presented to obtain a broad perspective of the social media big data analytics research topic. We classify the literature based on important aspects. This study also compares possible big data analytics techniques and their quality attributes. Moreover, we provide a discussion on the applications of social media big data analytics by highlighting the state-of-the-art techniques, methods, and the quality attributes of various studies. Open research challenges in big data analytics are described as well."
}

@article{Buczak2016ASO,
  title={A Survey of Data Mining and Machine Learning Methods for Cyber Security Intrusion Detection},
  author={Anna L. Buczak and Erhan Guven},
  journal={IEEE Communications Surveys \& Tutorials},
  year={2016},
  volume={18},
  pages={1153-1176}
}

@article{Larraaga2006MachineLI,
  title={Machine learning in bioinformatics},
  author={Pedro Larra{\~n}aga and Borja Calvo and Roberto Santana and Concha Bielza and Josu Galdiano and I{\~n}aki Inza and Jos{\'e} Antonio Lozano and Rub{\'e}n Arma{\~n}anzas and Guzm{\'a}n Santaf{\'e} and Aritz P{\'e}rez Mart{\'i}nez and V{\'i}ctor Robles},
  journal={Briefings in bioinformatics},
  year={2006},
  volume={7 1},
  pages={
          86-112
        }
}

@article{LAGO2019191,
title = "Learning and managing context enriched behavior patterns in smart homes",
journal = "Future Generation Computer Systems",
volume = "91",
pages = "191 - 205",
year = "2019",
issn = "0167-739X",
doi = "https://doi.org/10.1016/j.future.2018.09.004",
url = "http://www.sciencedirect.com/science/article/pii/S0167739X18307180",
author = "Paula Lago and Claudia Roncancio and Claudia Jiménez-Guarín",
keywords = "Context-awareness, Behavior learning, Frequent pattern mining, Smart homes, Stream mining, Pervasive computing",
abstract = "In a society with a growing population of elders, providing efficient and cost-effective long-term care has become central to reducing the economic and societal impact of this demographic shift. Part of the solution to improving the well-being of the elderly at home are smart homes. We examine the role of smart homes in monitoring the activities of the elderly, identifying safety hazards in the home, and understanding environmental changes that may correlate with the deterioration of cognitive and physical health. In this paper, we present LaPlace, a system used to manage context enriched behavior patterns learned in a smart home with sensing devices. LaPlace is based on a formal model to represent intelligible, context enriched behavior patterns, an online adaptive learning algorithm called TIMe which was created for learning such patterns. Context-awareness provides insights about behavior that may otherwise go unnoticed. TIMe is a one-pass algorithm that uses a stream processing model. The TIMe algorithm is presented in this paper along with an extensive evaluation of it using real life datasets."
}

@article{HENRIQUE2019226,
title = "Literature review: Machine learning techniques applied to financial market prediction",
journal = "Expert Systems with Applications",
volume = "124",
pages = "226 - 251",
year = "2019",
issn = "0957-4174",
doi = "https://doi.org/10.1016/j.eswa.2019.01.012",
url = "http://www.sciencedirect.com/science/article/pii/S095741741930017X",
author = "Bruno Miranda Henrique and Vinicius Amorim Sobreiro and Herbert Kimura",
keywords = "Financial time series prediction, Machine learning, Literature review, Main path analysis",
abstract = "The search for models to predict the prices of financial markets is still a highly researched topic, despite major related challenges. The prices of financial assets are non-linear, dynamic, and chaotic; thus, they are financial time series that are difficult to predict. Among the latest techniques, machine learning models are some of the most researched, given their capabilities for recognizing complex patterns in various applications. With the high productivity in the machine learning area applied to the prediction of financial market prices, objective methods are required for a consistent analysis of the most relevant bibliography on the subject. This article proposes the use of bibliographic survey techniques that highlight the most important texts for an area of research. Specifically, these techniques are applied to the literature about machine learning for predicting financial market values, resulting in a bibliographical review of the most important studies about this topic. Fifty-seven texts were reviewed, and a classification was proposed for markets, assets, methods, and variables. Among the main results, of particular note is the greater number of studies that use data from the North American market. The most commonly used models for prediction involve support vector machines (SVMs) and neural networks. It was concluded that the research theme is still relevant and that the use of data from developing markets is a research opportunity."
}

@article{KAMILARIS201870,
title = "Deep learning in agriculture: A survey",
journal = "Computers and Electronics in Agriculture",
volume = "147",
pages = "70 - 90",
year = "2018",
issn = "0168-1699",
doi = "https://doi.org/10.1016/j.compag.2018.02.016",
url = "http://www.sciencedirect.com/science/article/pii/S0168169917308803",
author = "Andreas Kamilaris and Francesc X. Prenafeta-Boldú",
keywords = "Deep learning, Agriculture, Survey, Convolutional Neural Networks, Recurrent Neural Networks, Smart farming, Food systems",
abstract = "Deep learning constitutes a recent, modern technique for image processing and data analysis, with promising results and large potential. As deep learning has been successfully applied in various domains, it has recently entered also the domain of agriculture. In this paper, we perform a survey of 40 research efforts that employ deep learning techniques, applied to various agricultural and food production challenges. We examine the particular agricultural problems under study, the specific models and frameworks employed, the sources, nature and pre-processing of data used, and the overall performance achieved according to the metrics used at each work under study. Moreover, we study comparisons of deep learning with other existing popular techniques, in respect to differences in classification or regression performance. Our findings indicate that deep learning provides high accuracy, outperforming existing commonly used image processing techniques."
}

 @misc{ wiki:RegressionAnalysis,
   author = "{Wikipedia contributors}",
   title = "Regression analysis",
   year = "2019",
   url = "https://en.wikipedia.org/wiki/Regression_analysis#History",
   note = "[Online; accessed 16-April-2019]"
 }

@misc{GunningXAI,
author = {{David Gunning}},
title = {Explainable Artificial Intelligence (XAI)},
type = {Blog},
year = {2016},
howpublished = {https://www.darpa.mil/program/explainable-artificial-intelligence},
urldate = {2019-04-19}
}

@article{Vandewiele2016GENESIMGE,
  title={GENESIM: genetic extraction of a single, interpretable model},
  author={Gilles Vandewiele and Olivier Janssens and Femke Ongenae and Filip De Turck and Sofie Van Hoecke},
  journal={CoRR},
  year={2016},
  volume={abs/1611.05722}
}

@article{Frosst2017DistillingAN,
  title={Distilling a Neural Network Into a Soft Decision Tree},
  author={Nicholas Frosst and Geoffrey E. Hinton},
  journal={CoRR},
  year={2017},
  volume={abs/1711.09784}
}
