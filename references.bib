@article{Interlandi2017,
author = {Interlandi, Matteo and Ekmekji, Ari and Shah, Kshitij and Gulzar, Muhammad Ali and Tetali, Sai Deep and Kim, Miryung and Millstein, Todd and Condie, Tyson},
journal = {VLDB Journal},
number = {5},
pages = {1--21},
title = {{Adding data provenance support to Apache Spark}},
volume = {27},
year = {2017}
}

@inproceedings{Ma2017,
 author = {Ma, Shiqing and Aafer, Yousra and Xu, Zhaogui and Lee, Wen-Chuan and Zhai, Juan and Liu, Yingqi and Zhang, Xiangyu},
 title = {{LAMP}: Data Provenance for Graph Based Machine Learning Algorithms Through Derivative Computation},
 booktitle = {Proceedings of the 2017 11th Joint Meeting on Foundations of Software Engineering},
 year = {2017},
 pages = {786--797}
} 

@article{Curcin2017,
author = {Curcin, Vasa and Fairweather, Elliot and Danger, Roxana and Corrigan, Derek},
journal = {Journal of Biomedical Informatics},
pages = {1--21},
title = {{Templates as a method for implementing data provenance in decision support systems}},
volume = {65},
year = {2017}
}

@article{MONTAVON20181,
title = {{Methods for interpreting and understanding deep neural networks}},
journal = "Digital Signal Processing",
volume = "73",
pages = "1 - 15",
year = "2018",
issn = "1051-2004",
doi = "https://doi.org/10.1016/j.dsp.2017.10.011",
url = "http://www.sciencedirect.com/science/article/pii/S1051200417302385",
author = "Grégoire Montavon and Wojciech Samek and Klaus-Robert Müller",
keywords = "Deep neural networks, Activation maximization, Sensitivity analysis, Taylor decomposition, Layer-wise relevance propagation",
abstract = "This paper provides an entry point to the problem of interpreting a deep neural network model and explaining its predictions. It is based on a tutorial given at ICASSP 2017. As a tutorial paper, the set of methods covered here is not exhaustive, but sufficiently representative to discuss a number of questions in interpretability, technical challenges, and possible applications. The second part of the tutorial focuses on the recently proposed layer-wise relevance propagation (LRP) technique, for which we provide theory, recommendations, and tricks, to make most efficient use of it on real data."
}

@article{Lapuschkin2016,
abstract = {The Layer-wise Relevance Propagation (LRP) algorithm explains a classifier's prediction specific to a given data point by attributing relevance scores to important components of the input by using the topology of the learned model itself. With the LRP Toolbox we provide platform-agnostic implementations for explaining the predictions of pre-trained state of the art Caffe networks and stand-alone implementations for fully connected Neural Network models. The implementations for Matlab and python shall serve as a playing field to familiarize oneself with the LRP algorithm and are implemented with readability and transparency in mind. Models and data can be imported and exported using raw text formats, Matlab's .mat files and the .npy format for numpy or plain text.},
author = {Lapuschkin, Sebastian and Binder, Alexander and M{\"{u}}ller, Klaus- Robert and Samek, Wojciech},
journal = {Journal of Machine Learning Research},
title = {{The LRP Toolbox for Artificial Neural Networks}},
url = {http://www.mendeley.com/research/lrp-toolbox-artificial-neural-networks},
year = {2016}
}

@inproceedings{Liu:2018:ADM:3219819.3220027,
 author = {Liu, Ninghao and Yang, Hongxia and Hu, Xia},
 title = {{Adversarial Detection with Model Interpretation}},
 booktitle = {Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery \& Data Mining},
 series = {KDD '18},
 year = {2018},
 isbn = {978-1-4503-5552-0},
 location = {London, United Kingdom},
 pages = {1803--1811},
 numpages = {9},
 url = {http://doi.acm.org/10.1145/3219819.3220027},
 doi = {10.1145/3219819.3220027},
 acmid = {3220027},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {adversarial detection, machine learning interpretation, spammer detection},
} 

@article{Wexler.2017,
 author  = {Wexler, Rebecca},
 date    = {2017-06-13},
 title   = {{When a Computer Program Keeps You in Jail}},
 journal = {The New York Times},
 url     = {https://www.nytimes.com/2017/06/13/opinion/how-computers-are-harming-criminal-justice.html},
 urldate = {2019-02-03},
 month = jun,
 year = {2017},
}

@article{Angwin2016,
author = {Angwin, Jula and Larson Jeff and Matta, Surya and Kirchner, Lauren},
date = {2016-05-23},
title = {Machine Bias},
journal = {Pro Publica},
year = {2016},
url = {https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing},
urldate = {2019-02-09},
}

@article{Hohman2018,
abstract = {Deep learning has recently seen rapid development and received significant attention due to its state-of-the-art performance on previously-thought hard problems. However, because of the internal complexity and nonlinear structure of deep neural networks, the underlying decision making processes for why these models are achieving such performance are challenging and sometimes mystifying to interpret. As deep learning spreads across domains, it is of paramount importance that we equip users of deep learning with tools for understanding when a model works correctly, when it fails, and ultimately how to improve its performance. Standardized toolkits for building neural networks have helped democratize deep learning; visual analytics systems have now been developed to support model explanation, interpretation, debugging, and improvement. We present a survey of the role of visual analytics in deep learning research, which highlights its short yet impactful history and thoroughly summarizes the state-of-the-art using a human-centered interrogative framework, focusing on the Five W's and How (Why, Who, What, How, When, and Where). We conclude by highlighting research directions and open research problems. This survey helps researchers and practitioners in both visual analytics and deep learning to quickly learn key aspects of this young and rapidly growing body of research, whose impact spans a diverse range of domains.},
archivePrefix = {arXiv},
arxivId = {1801.06889},
author = {Hohman, Fred Matthew and Kahng, Minsuk and Pienta, Robert and Chau, Duen Horng},
doi = {10.1109/TVCG.2018.2843369},
eprint = {1801.06889},
isbn = {10772626 (ISSN)},
issn = {10772626},
journal = {IEEE Transactions on Visualization and Computer Graphics},
keywords = {DNN,Deep learning,information visualization,neural networks,survey,visual analytics,visualization},
mendeley-groups = {Surveys},
mendeley-tags = {DNN,neural networks,survey,visualization},
number = {1},
pages = {1--20},
title = {{Visual Analytics in Deep Learning: An Interrogative Survey for the Next Frontiers}},
volume = {25},
year = {2018}
}

@inproceedings{Ribeiro:2016:WIT:2939672.2939778,
 author = {Ribeiro, Marco Tulio and Singh, Sameer and Guestrin, Carlos},
 title = {{\"Why Should I Trust You?\": Explaining the Predictions of Any Classifier}},
 booktitle = {Proceedings of the 22Nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
 series = {KDD '16},
 year = {2016},
 isbn = {978-1-4503-4232-2},
 location = {San Francisco, California, USA},
 pages = {1135--1144},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/2939672.2939778},
 doi = {10.1145/2939672.2939778},
 acmid = {2939778},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {black box classifier, explaining machine learning, interpretability, interpretable machine learning},
} 

@article{Guidotti:2018:SME:3271482.3236009,
 author = {Guidotti, Riccardo and Monreale, Anna and Ruggieri, Salvatore and Turini, Franco and Giannotti, Fosca and Pedreschi, Dino},
 title = {{A Survey of Methods for Explaining Black Box Models}},
 journal = {ACM Comput. Surv.},
 issue_date = {January 2019},
 volume = {51},
 number = {5},
 month = aug,
 year = {2018},
 issn = {0360-0300},
 pages = {93:1--93:42},
 articleno = {93},
 numpages = {42},
 url = {http://doi.acm.org/10.1145/3236009},
 doi = {10.1145/3236009},
 acmid = {3236009},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {Open the black box, explanations, interpretability, transparent models},
} 

@inproceedings{Veale:2018:FAD:3173574.3174014,
 author = {Veale, Michael and Van Kleek, Max and Binns, Reuben},
 title = {{Fairness and Accountability Design Needs for Algorithmic Support in High-Stakes Public Sector Decision-Making}},
 booktitle = {Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems},
 series = {CHI '18},
 year = {2018},
 isbn = {978-1-4503-5620-6},
 location = {Montreal QC, Canada},
 pages = {440:1--440:14},
 articleno = {440},
 numpages = {14},
 url = {http://doi.acm.org/10.1145/3173574.3174014},
 doi = {10.1145/3173574.3174014},
 acmid = {3174014},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {algorithmic accountability, algorithmic bias, decision-support, predictive policing, public administration},
}

@article{10.1371/journal.pone.0130140,
    author = {Bach, Sebastian AND Binder, Alexander AND Montavon, Grégoire AND Klauschen, Frederick AND Müller, Klaus-Robert AND Samek, Wojciech},
    journal = {PLOS ONE},
    publisher = {Public Library of Science},
    title = {On Pixel-Wise Explanations for Non-Linear Classifier Decisions by Layer-Wise Relevance Propagation},
    year = {2015},
    month = {07},
    volume = {10},
    url = {https://doi.org/10.1371/journal.pone.0130140},
    pages = {1-46},
    abstract = {Understanding and interpreting classification decisions of automated image classification systems is of high value in many applications, as it allows to verify the reasoning of the system and provides additional information to the human expert. Although machine learning methods are solving very successfully a plethora of tasks, they have in most cases the disadvantage of acting as a black box, not providing any information about what made them arrive at a particular decision. This work proposes a general solution to the problem of understanding classification decisions by pixel-wise decomposition of nonlinear classifiers. We introduce a methodology that allows to visualize the contributions of single pixels to predictions for kernel-based classifiers over Bag of Words features and for multilayered neural networks. These pixel contributions can be visualized as heatmaps and are provided to a human expert who can intuitively not only verify the validity of the classification decision, but also focus further analysis on regions of potential interest. We evaluate our method for classifiers trained on PASCAL VOC 2009 images, synthetic image data containing geometric shapes, the MNIST handwritten digits data set and for the pre-trained ImageNet model available as part of the Caffe open source package.},
    number = {7},
    doi = {10.1371/journal.pone.0130140}
}

@article{Goodman2017EuropeanUR,
  title={European Union Regulations on Algorithmic Decision-Making and a “Right to Explanation”},
  author={Bryce Goodman and Seth Flaxman},
  journal={AI Magazine},
  year={2017},
  volume={38},
  pages={50-57}
}

@inproceedings{Bibal2016,
author = {Bibal, Adrien and Frénay, Benoît},
year = {2016},
month = {04},
pages = {},
title = {Interpretability of Machine Learning Models and Representations: an Introduction},
booktitle = {European Symposium on Artificial Neural Networks, Computational Intelligence and Machine Learning},
 }

@InProceedings{10.1007/978-3-319-46493-0_1,
author="Hendricks, Lisa Anne
and Akata, Zeynep
and Rohrbach, Marcus
and Donahue, Jeff
and Schiele, Bernt
and Darrell, Trevor",
editor="Leibe, Bastian
and Matas, Jiri
and Sebe, Nicu
and Welling, Max",
title="Generating Visual Explanations",
booktitle="Computer Vision -- ECCV 2016",
year="2016",
publisher="Springer International Publishing",
address="Cham",
pages="3--19",
abstract="Clearly explaining a rationale for a classification decision to an end user can be as important as the decision itself. Existing approaches for deep visual recognition are generally opaque and do not output any justification text; contemporary vision-language models can describe image content but fail to take into account class-discriminative image aspects which justify visual predictions. We propose a new model that focuses on the discriminating properties of the visible object, jointly predicts a class label, and explains why the predicted label is appropriate for the image. Through a novel loss function based on sampling and reinforcement learning, our model learns to generate sentences that realize a global sentence property, such as class specificity. Our results on the CUB dataset show that our model is able to generate explanations which are not only consistent with an image but also more discriminative than descriptions produced by existing captioning methods.",
isbn="978-3-319-46493-0"
}

@inproceedings{Nguyen2016,
archivePrefix = {arXiv},
arxivId = {1605.09304},
author = {Nguyen, Anh and Dosovitskiy, Alexey and Yosinski, Jason and Brox, Thomas and Clune, Jeff},
booktitle = {NIPS},
doi = {10.1098/rstb.2009.0091},
eprint = {1605.09304},
file = {:Users/david/Library/Application Support/Mendeley Desktop/Downloaded/Nguyen et al. - 2016 - Synthesizing the preferred inputs for neurons in neural networks via deep generator networks.pdf:pdf},
issn = {10495258},
keywords = {DNN,activation maximization,interpretability},
mendeley-tags = {DNN,activation maximization,interpretability},
title = {{Synthesizing the preferred inputs for neurons in neural networks via deep generator networks}},
year = {2016}
}

@inproceedings{Abdul:2018:TTE:3173574.3174156,
 author = {Abdul, Ashraf and Vermeulen, Jo and Wang, Danding and Lim, Brian Y. and Kankanhalli, Mohan},
 title = {Trends and Trajectories for Explainable, Accountable and Intelligible Systems: An {HCI} Research Agenda},
 booktitle = {Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems},
 series = {CHI '18},
 year = {2018},
 isbn = {978-1-4503-5620-6},
 location = {Montreal QC, Canada},
 pages = {582:1--582:18},
 articleno = {582},
 numpages = {18},
 url = {http://doi.acm.org/10.1145/3173574.3174156},
 doi = {10.1145/3173574.3174156},
 acmid = {3174156},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {explainable artificial intelli-gence, explanations, intelligibility, interpretable machine learning},
} 

@article{Berendt2014,
author={Berendt, Bettina and Preibusch, S{\"o}ren},
title={{Better decision support through exploratory discrimination-aware data mining: foundations and empirical evidence}},
journal="Artificial Intelligence and Law",
year="2014",
month="Jun",
day="01",
volume="22",
number="2",
pages="175--209",
abstract="Decision makers in banking, insurance or employment mitigate many of their risks by telling ``good'' individuals and ``bad'' individuals apart. Laws codify societal understandings of which factors are legitimate grounds for differential treatment (and when and in which contexts)---or are considered unfair discrimination, including gender, ethnicity or age. Discrimination-aware data mining (DADM) implements the hope that information technology supporting the decision process can also keep it free from unjust grounds. However, constraining data mining to exclude a fixed enumeration of potentially discriminatory features is insufficient. We argue for complementing it with exploratory DADM, where discriminatory patterns are discovered and flagged rather than suppressed. This article discusses the relative merits of constraint-oriented and exploratory DADM from a conceptual viewpoint. In addition, we consider the case of loan applications to empirically assess the fitness of both discrimination-aware data mining approaches for two of their typical usage scenarios: prevention and detection. Using Mechanical Turk, 215 US-based participants were randomly placed in the roles of a bank clerk (discrimination prevention) or a citizen / policy advisor (detection). They were tasked to recommend or predict the approval or denial of a loan, across three experimental conditions: discrimination-unaware data mining, exploratory, and constraint-oriented DADM (eDADM resp. cDADM). The discrimination-aware tool support in the eDADM and cDADM treatments led to significantly higher proportions of correct decisions, which were also motivated more accurately. There is significant evidence that the relative advantage of discrimination-aware techniques depends on their intended usage. For users focussed on making and motivating their decisions in non-discriminatory ways, cDADM resulted in more accurate and less discriminatory results than eDADM. For users focussed on monitoring for preventing discriminatory decisions and motivating these conclusions, eDADM yielded more accurate results than cDADM.",
issn="1572-8382",
doi="10.1007/s10506-013-9152-0",
url="https://doi.org/10.1007/s10506-013-9152-0"
}

@article{DBLP:journals/corr/abs-1711-00399,
  author    = {Sandra Wachter and
               Brent D. Mittelstadt and
               Chris Russell},
  title     = {Counterfactual Explanations without Opening the Black Box: Automated
               Decisions and the {GDPR}},
  journal   = {CoRR},
  volume    = {abs/1711.00399},
  year      = {2017},
  url       = {http://arxiv.org/abs/1711.00399},
  archivePrefix = {arXiv},
  eprint    = {1711.00399},
  timestamp = {Wed, 23 Jan 2019 13:31:00 +0100},
  biburl    = {https://dblp.org/rec/bib/journals/corr/abs-1711-00399},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{Adadi2018, 
author={A. Adadi and M. Berrada}, 
journal={IEEE Access}, 
title={Peeking Inside the Black-Box: A Survey on Explainable Artificial Intelligence (XAI)}, 
year={2018}, 
volume={6}, 
number={}, 
pages={52138-52160}, 
keywords={artificial intelligence;AI-based systems;black-box nature;explainable AI;XAI;explainable artificial intelligence;fourth industrial revolution;Conferences;Machine learning;Market research;Prediction algorithms;Machine learning algorithms;Biological system modeling;Explainable artificial intelligence;interpretable machine learning;black-box models}, 
doi={10.1109/ACCESS.2018.2870052}, 
ISSN={2169-3536}, 
month={},
}

@article{Read2016,
author = {Read, Richard},
date = {2016-03-01},
year = {2016},
title = {For the first time, Google admits its autonomous car is at fault in fender-bender},
journal = {Washington Post},
url = {https://www.washingtonpost.com/cars/for-the-first-time-google-admits-its-autonomous-car-is-at-fault-in-fender-bender/2016/03/01/356445c8-e009-11e5-8c00-8aa03741dced_story.html},
urldate = {2019-02-10},
}

@misc{Tesla2018,
author = {{The Tesla Team}},
title = {An Update on Last Week’s Accident},
journal = {Tesla Blog},
type = {Blog},
date = {2018-03-03},
number = {March 3},
year = {2018},
howpublished = {https://www.tesla.com/blog/update-last-week%E2%80%99s-accident},
urldate = {2019-02-10}
}

@article{Ackerman2016,
author = {Ackerman, Evan},
title = {Fatal Tesla Self-Driving Car Crash Reminds Us That Robots Aren't Perfect},
journal = {{IEEE} Spectrum},
date = {2016-06-01},
year = {2016},
url = {https://spectrum.ieee.org/cars-that-think/transportation/self-driving/fatal-tesla-autopilot-crash-reminds-us-that-robots-arent-perfect},
urldate = {2019-02-10}
}

@techreport{Bhavsar2017,
author = {Bhavsar, Parth and Dey, Kakan and Chowdhury, Mashrur and Das, Plaban},
title = {{Risk Analysis of Autonomous Vehicles in Mixed Traffic Streams}},
year = {2017}
}

@inproceedings{Schelter2017,
  author    = {Schelter, Sebastian and B{\"o}se, Joos-Hendrik and Klein, Thoralf and Seufert, Stephan},
  title     = {{Automatically Tracking Metadata and Provenance of Machine Learning Experiments}},
  year      = {2017},
  maintitle = {Conference on Neural Information Processing Systems},
  booktitle = {Machine Learning Systems Workshop},
}

@Article{Koo2015,
author={Koo, Jeamin
and Kwac, Jungsuk
and Ju, Wendy
and Steinert, Martin
and Leifer, Larry
and Nass, Clifford},
title={Why did my car just do that? Explaining semi-autonomous driving actions to improve driver understanding, trust, and performance},
journal={International Journal on Interactive Design and Manufacturing (IJIDeM)},
year={2015},
month={Nov},
day={01},
volume={9},
number={4},
pages={269--275},
abstract={This study explores, in the context of semi-autonomous driving, how the content of the verbalized message accompanying the car's autonomous action affects the driver's attitude and safety performance. Using a driving simulator with an auto-braking function, we tested different messages that provided advance explanation of the car's imminent autonomous action. Messages providing only ``how'' information describing actions (e.g., ``The car is braking'') led to poor driving performance, whereas ``why'' information describing reasoning for actions (e.g., ``Obstacle ahead'') was preferred by drivers and led to better driving performance. Providing both ``how and why'' resulted in the safest driving performance but increased negative feelings in drivers. These results suggest that, to increase overall safety, car makers need to attend not only to the design of autonomous actions but also to the right way to explain these actions to the drivers.},
issn={1955-2505},
doi={10.1007/s12008-014-0227-2},
url={https://doi.org/10.1007/s12008-014-0227-2}
}

@article{DBLP:journals/corr/abs-1812-00891,
  author    = {Xinyang Zhang and
               Ningfei Wang and
               Shouling Ji and
               Hua Shen and
               Ting Wang},
  title     = {Interpretable Deep Learning under Fire},
  journal   = {CoRR},
  volume    = {abs/1812.00891},
  year      = {2018},
  url       = {http://arxiv.org/abs/1812.00891},
  archivePrefix = {arXiv},
  eprint    = {1812.00891},
  timestamp = {Tue, 01 Jan 2019 15:01:25 +0100},
  biburl    = {https://dblp.org/rec/bib/journals/corr/abs-1812-00891},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{Chen2018,
  author    = {Chaofan Chen and
               Oscar Li and
               Alina Barnett and
               Jonathan Su and
               Cynthia Rudin},
  title     = {This looks like that: deep learning for interpretable image recognition},
  journal   = {CoRR},
  volume    = {abs/1806.10574},
  year      = {2018},
  url       = {http://arxiv.org/abs/1806.10574},
  archivePrefix = {arXiv},
  eprint    = {1806.10574},
  timestamp = {Mon, 13 Aug 2018 16:47:37 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/abs-1806-10574},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{Werbos1974,
author = {Werbos, Paul and J. (Paul John, Paul},
year = {1974},
month = {01},
pages = {},
title = {Beyond regression : new tools for prediction and analysis in the behavioral sciences /}
}

@article{kim2018textual,
  title={Textual Explanations for Self-Driving Vehicles},
  author={Kim, Jinkyu and Rohrbach, Anna and Darrell, Trevor and Canny, John and Akata, Zeynep},
  journal={Proceedings of the European Conference on Computer Vision (ECCV)},
  year={2018}
}

@article{Kim2017InterpretableLF,
  title={Interpretable Learning for Self-Driving Cars by Visualizing Causal Attention},
  author={Jinkyu Kim and John F. Canny},
  journal={2017 IEEE International Conference on Computer Vision (ICCV)},
  year={2017},
  pages={2961-2969}
}

@inproceedings{Kim2017ShowA,
  title={Show , Attend , Control , and Justify : Interpretable Learning for Self-Driving Cars},
  author={Jinkyu Kim and Anna Rohrbach and Trevor Darrell and John F. Canny and Zeynep Akata},
  year={2017}
}

@article{Bojarski2017ExplainingHA,
  title={Explaining How a Deep Neural Network Trained with End-to-End Learning Steers a Car},
  author={Mariusz Bojarski and Philip Yeres and Anna Choromanska and Krzysztof Choromanski and Bernhard Firner and Lawrence D. Jackel and Urs Muller},
  journal={CoRR},
  year={2017},
  volume={abs/1704.07911}
}

@article{BachBMMS15,
  author    = {Sebastian Bach and
               Alexander Binder and
               Gr{\'{e}}goire Montavon and
               Klaus{-}Robert M{\"{u}}ller and
               Wojciech Samek},
  title     = {Analyzing Classifiers: Fisher Vectors and Deep Neural Networks},
  journal   = {CoRR},
  volume    = {abs/1512.00172},
  year      = {2015},
  url       = {http://arxiv.org/abs/1512.00172},
  archivePrefix = {arXiv},
  eprint    = {1512.00172},
  timestamp = {Mon, 13 Aug 2018 16:47:18 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/BachBMMS15},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
