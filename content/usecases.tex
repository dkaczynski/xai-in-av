\section{Use Cases} \label{sec:UseCases}

Various methods of XAI have been layed out in the previous section, but the question still remains of how do these methods provide value to scientists, consumers, or society at large.  Here, we explore the various users who interface with XAI and what value is provided.

\subsection{As a data scientist, I can use the explanation of a model's decisions to identify opportunities to improve the training data set and make the model more robust}

This use case takes an \textit{a posteriori} approach to identifying opportunities to improve a model's performance.  In current literature, the XAI methods involved are commonly visualization techniques, but other methods are still applicable.  Due to the subjective nature of interpretation methods like visualization of relevant features, any action in improving the training of a model requires a human expert to analyze the explanation for potential weaknesses in the model's decision process.

In one example of improving a model through the identification of relevant features, a group of researchers used a Fisher Vector classifier on the PASCAL VOC 2007 data set \cite{Bach2016AnalyzingCF}.  After applying LRP to as a tool in explaining the classifier's decisions, it was apparent that the copyright watermark on images of horses was highly relevant in the Fisher Vector model.  The researchers then removed the copyright watermark before retraining the model, and future decisions considered the horse and its surroundings as more relevant features.

\begin{itemize}
    \item Liu et al focus on interpreting the decisions of a model used to identify cyber security threats \cite{Liu:2018:ADM:3219819.3220027}.  The researchers interpret the model's decisions that failed to successfully label threats.  The interpretations help the researchers identify how to generate specific perturbations in the training data that improve the training of the model against previously misidentified threats.  The accuracy of the model improves.
    \item LAMP is a tool that can trace the decision of a trained model to the importance that individual samples had on that decision. \cite{Ma2017}
\end{itemize}

\subsection{As a potential consumer of an AI/ML product, such as an autonomous vehicle or virtual assistant, an explanation of how the product is making its decisions will improve my trust in the product}

The psychological study of the trustworthiness of AI systems is as old as AI itself.  There are lots of concepts here; check out some of Mueller's work on XAI (big paper in Mendeley).  THe two papers in the outline here are great examples, but it would be good to tie it back into HCI/HMI/psychology of AI systems, right?  I think there are actually quite a few papers like that in Mendeley, too, in the "XAI \& AV" directory.

\begin{itemize}
    \item Human subjects are asked about how much they trust a classifier before and after its explanations are made available \cite{Ribeiro:2016:WIT:2939672.2939778}.  The image of a dog was misclassified as a wolf via an intentionally bad model, and a small set of human subjects trusted the classifier more after it was revealed that snow was apparently a feature rather than the shape, size, or color of the animal.
    \item Decisions from autonomous vehicles and advanced driver assistance systems (ADAS) may be explained using XAI methods to answer the questions "How?" and "Why?".  Koo et al measure both the emotional impact and impact in making safer decisions when drivers are provided answers to these questions \cite{Koo2015}
\end{itemize}

\subsection{As a provider of an AI/ML product, I am responsible for providing an explanation for how my product makes its decisions in the case of being the subject of scrutiny, legal or otherwise}

While legislation such as the EU's General Data Protection Regulation's "right to explanation" faces legitimate criticism about its scope and ability to be enforced \cite{10.1093/idpl/ipx005}, there exist non-legal reasons as well for providers of AI/ML products to provide explanations for their systems' decisions.  Products and services infused with machine learning, such as autonomous vehicles, face challenges in establishing trust with consumers, and a product that has the ability to provide human-interpretable explanations for its decisions may gain a market advantage over its competitors.  Autonomous vehicles and vehicles with ADAS from Tesla, Google, and GM Cruise have been involved with numerous traffic incidents, ranging from trivial to fatal \cite{Read2016} \cite{Tesla2018} \cite{Ackerman2016} \cite{Bhavsar2017}.  There also exist an ethical directive to identify discriminatory bias in ML systems.  For example, in the criminal justice system, judges and parole boards may apply predictive models as tools in making their decisions, but racial discrimination has been uncovered in such tools \cite{Wexler.2017} \cite{Angwin2016}.

\begin{itemize}
    \item Human subjects conduct fictitious banking scenarios, both with and without discrimination-aware data mining (DADM) tools \cite{Berendt2014}.  The accuracy and presence of discrimination in decisions was compared across tools.

    \item A magazine article describes the GPDR's "right to explanation" and includes a survey of research in identifying and rectifying discrimination \cite{Goodman2017EuropeanUR}
\end{itemize}

