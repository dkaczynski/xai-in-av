\begin{abstract}
Artificial intelligence and, especially, machine learning have gained notoriety in recent years due to the influx in ways that they impact people's lives.  AI/ML can replace and supplement human experts and provide automation in ways like never before, such as medical diagnosis, smart home technology, and autonomous vehicles.  With AI/ML adopting roles like these so close to the personal lives of the general public, there has been growing concern from political and technological leaders about putting too much trust in the automated decision-making processes or about the biases that they can inadvertently learn via training.  The internals of some AI/ML methods are inherently difficult to interpret, e.g. deep neural networks.  The combination of the societal interest in making AI/ML systems more accountable combined with the difficult-to-interpret nature of some automated decision systems has brought about a new field of research frequently referred to as explainable artificial intelligence, or XAI.  There is currently sparse literature on the overlap between XAI and autonomous vehicles.  In this survey, the relationship between the demand for and value provided by XAI is explored to define cohesive use cases, and we align those use cases with the various needs of engineers, consumers, and auditors of autonomous vehicle technology.
\end{abstract}
