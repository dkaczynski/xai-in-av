\section{Alignment with Autonomous Vehicles}\label{sec:Alignment}

Machine learning is all up in autonomous vehicles.  It is used in the perception tasks such as segmentation of entities from sensor data.  Machine learning is also used in the decision layer of the autonomous vehicle that chooses the safest route to the destination.  Where ever there is machine learning, there is an opportunity to apply XAI.

The three use cases defined in \ref{sec:UseCases} each can be applied in the context of autonomous vehicles.  The development process of the ML systems in the vehicle can be strengthened by data provenance techniques that improve collaboration.  Potential consumers of autonomous vehicles may feel safer when the vehicle can provide verbal explanations of the decisions that it is making.  And automakers may be inclined or, potentially, required to be able to provide explanations to legal or auditory queries for their autonomous systems.

There is existing research in the field of applying XAI methods in the space of autonomous vehicles, but there is much uncertainty in the scope of the relationship between these two areas.  Legal responsibilities are sometimes unclear or poorly defined for autonomous vehicle manufacturers, and the relationship between humans and AI systems is ever evolving.  It is important for researchers to forge ahead on incorporating XAI concepts in autonomous vehicles before the demand arises, not in response to it.

\subsection{Existing Research}

*Really* need to just write these summaries in natural language here.  There's a good start in the outline!!!  Take those, relate them back to the use cases and background in previous sections.

\begin{itemize}
    \item Textual Explanations for Self-Driving Vehicles \cite{kim2018textual}: This is the most comprehensive paper from this group of authors.  Very great work!  Explanations for the vehicle's decisions are presented to human subjects for validation.

    \item Interpretable Learning for Self-Driving Cars by Visualizing Causal Attention \cite{Kim2017InterpretableLF}:  This paper focuses more on just the visual interpretation of the agent's choices.

    \item Show, Attend, Control, and Justify: Interpretable Learning for Self-Driving Cars \cite{Kim2017ShowA}: This paper describes the LSTM topology for generating textual explanations

    \item Why did my car just do that? Explaining semi-autonomous driving actions to improve driver understanding, trust, and performance \cite{Koo2015}:  Human subjects act as drivers in HIL simulation of a semi-autonomous vehicle that can create hard brake events in emergency scenarios.
    
    \item Explaining How a Deep Neural Network Trained with End-to-End Learning Steers a Car \cite{Bojarski2017ExplainingHA}: There's one more article by these authors (currently in our Mendeley)...one is short and visual, the other is highly theoretical and explains in detail the process for generating saliency maps
\end{itemize}

\subsection{Gaps and Method Alignment}

How do we approach this section???  Potential idea:  talk about each of the three use cases in turn, addressing which items in the background can be applied to that use case that haven't been done yet.  *Demonstrate the need or value of doing so*.

\begin{itemize}
    \item There are a couple papers about the multimodal sensors in AV, the perception/decision architecture, and a survey of decision 
    \item There are quite a few papers about establishing trust with automated systems, AI, and HCI.  Is there a gap in establishing trust that has not been addressed in previous research?
    \item Might have to put the legal/auditory use case on the back burner...way out of scope for my expertise at this time.
\end{itemize}
