\section{Conclusion}

XAI is a relatively young yet increasingly popular field of research in which researchers of heterogenous backgrounds and goals struggle to find cohesion and unification.  An evolving vocabulary makes it difficult for similar research to be identified and related to one another.  The subjective and abstract concepts of interpretation and explanation make it difficult to quantify characteristics of models and their explanations.  And the diverse array of stakeholders from a variety of backgrounds makes it difficult to design systems that support the interfaces, data collection, and explanation methods necessary to create a cohesive concept of an explainable and accountable system.

"Black box" machine learning models are pervasive in the development of autonomous vehicles.  They are used in perception tasks, such as segmentation of entities from sensor data.  Machine learning is also applied in the decision system of the autonomous vehicle by responding to continuous traffic scenarios to navigate to the final route destination.  Where ever there is machine learning, there is an opportunity to apply methods of XAI, such as feature relevancy, explanation verbalization, extraction of explicit rules, and data provenance.  While there exists literature on applying XAI methods in the context of autonomous vehicles, there is no research yet on how it may provide value to the variety of stakeholders.

The three use cases defined in section \ref{sec:UseCases} each can be applied in the context of autonomous vehicles, and each use case pertains to one of three groups of users: developers and engineers, consumers, and auditory and investigatory entities.  The development process of the ML systems in the vehicle can be strengthened by data provenance techniques that improve collaboration or also by using explanations to identify opportunities to improve predictive models.  Potential consumers of autonomous vehicles may feel safer when the vehicle can provide verbal explanations of the decisions that it is making.  And automakers may be inclined or, potentially, required to be able to provide explanations to legal or auditory queries for the decisions made by their autonomous systems.  Existing literature demonstrates how XAI methods can be applied to provide insights to developers and establish trust with end users, but there is a lack of literature on the ethical and legal obligation to provide explanations of the decisions made by autonomous vehicles.

